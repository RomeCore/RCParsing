import{_ as r}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,a as n,o}from"./app-B_297w9j.js";const a={};function i(s,e){return o(),t("div",null,[...e[0]||(e[0]=[n('<p><code>RCParsing</code> uses lexerless approach, but it has <em>tokens</em>, they are used as parser primitives, and they are not complex as <em>rules</em>, having a limited error handling and configuration. This library also supports <em>barrier tokens</em>, unlocking hybrid mode.</p><h3 id="token" tabindex="-1"><a class="header-anchor" href="#token"><span>Token</span></a></h3><p>A <strong>Token</strong> is the smallest matching unit, a primitive that consumes input and produces an intermediate value. Tokens are ideal for:</p><ul><li>Terminal symbols (keywords, operators: <code>&quot;if&quot;</code>, <code>&quot;+&quot;</code>) and atomic values (numbers, strings, identifiers).</li><li>Complex patterns with immediate value transformation and zero-allocation (combinator-style).</li></ul><p><strong>Key trait:</strong> They cannot reference Rules, making them the foundation for all parsing. And they <strong>cannot</strong> contain inner barrier tokens.</p><h3 id="rule" tabindex="-1"><a class="header-anchor" href="#rule"><span>Rule</span></a></h3><p>A <strong>Rule</strong> is a composite structure built from Tokens and other Rules. It defines the grammar&#39;s structure and produces an Abstract Syntax Tree (AST) node. Rules are used for:</p><ul><li>Non-terminal symbols (expressions, statements, blocks).</li><li>Organizing the grammar hierarchy (<code>expression</code> -&gt; <code>term</code> -&gt; <code>factor</code>).</li><li>Constructing the AST with <code>Transform</code> functions for semantic analysis.</li></ul><p><strong>Key trait:</strong> They form the recursive structure of your grammar and build the parse tree.</p><h3 id="barrier-token" tabindex="-1"><a class="header-anchor" href="#barrier-token"><span>Barrier Token</span></a></h3><p>A <strong>Barrier Token</strong> is a special pseudo-token injected by a <code>BarrierTokenizer</code> (like the indent/dedent tokenizer). They are not defined in the grammar but are crucial for parsing context-sensitive syntax.</p><ul><li><strong>Purpose:</strong> To handle structures where traditional tokenization is insufficient.</li><li><strong>Primary Example:</strong> Parsing indentation-based languages (Python, YAML). The tokenizer analyzes whitespace to emit <code>INDENT</code> <code>DEDENT</code> and optinal <code>NEWLINE</code> tokens, which act as explicit markers for block boundaries, mimicking the role of <code>{</code> and <code>}</code> in other languages.</li></ul><p><strong>Key trait:</strong> They are generated and baked entirely before parsing, enabling recognition of non-regular structures.</p>',13)])])}const d=r(a,[["render",i]]),m=JSON.parse('{"path":"/guide/main_concepts.html","title":"Main concepts","lang":"en-US","frontmatter":{"title":"Main concepts","icon":"material-symbols:question-mark"},"git":{"createdTime":1758729324000,"updatedTime":1758729324000,"contributors":[{"name":"RomeCore","username":"RomeCore","email":"62770895+RomeCore@users.noreply.github.com","commits":1,"url":"https://github.com/RomeCore"}]},"readingTime":{"minutes":0.92,"words":277},"filePathRelative":"guide/main_concepts.md"}');export{d as comp,m as data};
